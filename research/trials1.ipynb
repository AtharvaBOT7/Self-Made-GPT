{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f032ad39",
   "metadata": {},
   "source": [
    "## Pre-Requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f3ad3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79c9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.getenv(\"input_path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eceff477",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (input_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2631901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the input text:  1115393\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the input text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b142ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 500 characters of the input text: \n",
      "\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(\"First 500 characters of the input text: \\n\\n\")\n",
    "print(text[:500])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4f39d8",
   "metadata": {},
   "source": [
    "## Building the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b250076a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"\".join(chars))\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b385d3",
   "metadata": {},
   "source": [
    "### Tokenization Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d64315",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # This is the encoder part, it takes a string as input and returns a list of integers as output\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # This is the decoder part, it takes a list of integers as input and returns a string as output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f35cf",
   "metadata": {},
   "source": [
    "For this research we are using a simple character level tokenizer and not the sub-word level tokenizer like the tiktoken or sentenepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21461a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 1, 39, 51, 1, 13, 58, 46, 39, 56, 60, 39]\n",
      "I am Atharva\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"I am Atharva\"))\n",
    "print(decode(encode(\"I am Atharva\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900063fa",
   "metadata": {},
   "source": [
    "So we basically have used our vocabulary size and mapped different character with numbers and then we encode it using that mapping and similarly decode it using the reverse logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d82f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3b5ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde953c",
   "metadata": {},
   "source": [
    "## Train / Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9ceb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1027ec8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003853, 111540)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77d8e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # This is the size of the data that we will pass in one pass to the transformer model\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe651218",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c15c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([18]), the target is 47\n",
      "When the input is tensor([18, 47]), the target is 56\n",
      "When the input is tensor([18, 47, 56]), the target is 57\n",
      "When the input is tensor([18, 47, 56, 57]), the target is 58\n",
      "When the input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When the input is {context}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa31e53",
   "metadata": {},
   "source": [
    "So if we give the input character length of 1, then our transformer can predict and it can predict till we give length equal to block size but if we give more than that, it will then start truncating the input as the maximum input is 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e048534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Using Metal GPU\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'mps':\n",
    "    print(f\"Using Metal GPU\")\n",
    "else:\n",
    "    print(f\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d62ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]], device='mps:0')\n",
      "\n",
      "\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]], device='mps:0')\n",
      "----\n",
      "\n",
      "\n",
      "When input is: [53], the target is: 59\n",
      "When input is: [53, 59], the target is: 6\n",
      "When input is: [53, 59, 6], the target is: 1\n",
      "When input is: [53, 59, 6, 1], the target is: 58\n",
      "When input is: [53, 59, 6, 1, 58], the target is: 56\n",
      "When input is: [53, 59, 6, 1, 58, 56], the target is: 47\n",
      "When input is: [53, 59, 6, 1, 58, 56, 47], the target is: 40\n",
      "When input is: [53, 59, 6, 1, 58, 56, 47, 40], the target is: 59\n",
      "When input is: [49], the target is: 43\n",
      "When input is: [49, 43], the target is: 43\n",
      "When input is: [49, 43, 43], the target is: 54\n",
      "When input is: [49, 43, 43, 54], the target is: 1\n",
      "When input is: [49, 43, 43, 54, 1], the target is: 47\n",
      "When input is: [49, 43, 43, 54, 1, 47], the target is: 58\n",
      "When input is: [49, 43, 43, 54, 1, 47, 58], the target is: 1\n",
      "When input is: [49, 43, 43, 54, 1, 47, 58, 1], the target is: 58\n",
      "When input is: [13], the target is: 52\n",
      "When input is: [13, 52], the target is: 45\n",
      "When input is: [13, 52, 45], the target is: 43\n",
      "When input is: [13, 52, 45, 43], the target is: 50\n",
      "When input is: [13, 52, 45, 43, 50], the target is: 53\n",
      "When input is: [13, 52, 45, 43, 50, 53], the target is: 8\n",
      "When input is: [13, 52, 45, 43, 50, 53, 8], the target is: 0\n",
      "When input is: [13, 52, 45, 43, 50, 53, 8, 0], the target is: 26\n",
      "When input is: [1], the target is: 39\n",
      "When input is: [1, 39], the target is: 1\n",
      "When input is: [1, 39, 1], the target is: 46\n",
      "When input is: [1, 39, 1, 46], the target is: 53\n",
      "When input is: [1, 39, 1, 46, 53], the target is: 59\n",
      "When input is: [1, 39, 1, 46, 53, 59], the target is: 57\n",
      "When input is: [1, 39, 1, 46, 53, 59, 57], the target is: 43\n",
      "When input is: [1, 39, 1, 46, 53, 59, 57, 43], the target is: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"\\n\\nTargets: \")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\\n\\n\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When input is: {context.tolist()}, the target is: {target.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b839f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b895b5",
   "metadata": {},
   "source": [
    "This is the actual data that we are going to feed to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c464646",
   "metadata": {},
   "source": [
    "## Transformer Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c887f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create embedding table: maps each character to a vector of size vocab_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        # Get embeddings for input tokens\n",
    "        logits = self.token_embedding_table(idx) # Currently in the shape (B, T, C)\n",
    "\n",
    "        # If no targets provided, we're just generating (no loss needed)\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # We are changing the shape of logits from (B, T, C) to (B*T, C) so that we can compute the loss using nn.CrossEntropyLoss, converting it to 2 dimensions so that the channel \n",
    "            # dimension is the second dimension as per the requirement of nn.CrossEntropyLoss and we will do the same for targets as well.\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Generate max_new_tokens new characters one at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get predictions from the model\n",
    "            logits, loss = self(idx)\n",
    "\n",
    "            # Focus only on the last time step's predictions\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "\n",
    "            # Append the sampled token to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9211fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7525, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create model and move to Metal GPU\n",
    "m = BigramLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Move data batches to Metal GPU\n",
    "xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "# Get predictions and loss\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)  # (batch_size, block_size, vocab_size)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e9ff2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UwasU3TKMYMj-fEcqNPVQbRe.OGOuUfZEiewNy::dl-jkczCOIiHeg EggeuTpbDbYhMYVcoS:rXcuSrng&?ofeOAZrYftKyLXcz\n"
     ]
    }
   ],
   "source": [
    "# Generate 100 new tokens starting from a newline character (index 0)\n",
    "# Create starting tensor directly on Metal device\n",
    "start_idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated = m.generate(idx=start_idx, max_new_tokens=100)\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861085e3",
   "metadata": {},
   "source": [
    "## Creating a PyTorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0831cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9387b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ea171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4f21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bc8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabda3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceeffbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf153ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acfaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9cba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
